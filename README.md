## Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development |  |
| Validator type | Brand risk |
| Blog |  |
| License | Apache 2 |
| Input/Output | Output |

## Description

This validator ensures that there’s no profanity in any generated text. This validator uses the `alt-profanity-check` package to check if a string contains profanity language.

### Intended use

This validator catches profanity in the English language only

### Resources required

- Dependencies: `alt-profanity-check`

## Installation

```bash
$ gudardrails hub install hub://guardrails/profanity_free
```

## Usage Examples

### Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import ProfanityFree
from guardrails import Guard

# Initialize Validator
val = ProfanityFree(on_fail="noop")

# Setup Guard
guard = Guard.from_string(validators=[val, ...])

guard.parse("dog")  # Validator passes
guard.parse("horse")  # Validator fails
```

### Validating JSON output via Python

In this example, we apply the validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import ProfanityFree
from guardrails import Guard

# Initialize Validator
val = ProfanityFree(on_fail="noop")

# Create Pydantic BaseModel
class LLMOutput(BaseModel):
    output: str = Field(
        description="Output generated by LLM", validators=[val]
    )

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=LLMOutput)

# Run LLM output generating JSON through guard
guard.parse("""
{
		"output": "Some output without profanity",
}
""")
```

## API Reference

`__init__`
- `on_fail`: The policy to enact when a validator fails.
